{"version":3,"sources":["../src/ollama.ts"],"names":[],"mappings":";;;;AAwBA,IAAM,WAAA,GAAc,wBAAA;AACpB,IAAM,aAAA,GAAgB,mBAAA;AACtB,IAAM,eAAA,GAAkB,GAAA;AAMjB,SAAS,qBAAA,CAAsB,MAAA,GAAuB,EAAC,EAAiB;AAC7E,EAAA,MAAM,GAAA,GAAM,OAAO,GAAA,IAAO,WAAA;AAC1B,EAAA,MAAM,KAAA,GAAQ,OAAO,KAAA,IAAS,aAAA;AAC9B,EAAA,MAAM,OAAA,GAAU,OAAO,OAAA,IAAW,eAAA;AAElC,EAAA,OAAO,OAAO,MAAA,KAAoC;AAChD,IAAA,MAAM,UAAA,GAAa,IAAI,eAAA,EAAgB;AACvC,IAAA,MAAM,YAAY,UAAA,CAAW,MAAM,UAAA,CAAW,KAAA,IAAS,OAAO,CAAA;AAE9D,IAAA,IAAI;AACF,MAAA,MAAM,QAAA,GAAW,MAAM,KAAA,CAAM,CAAA,EAAG,GAAG,CAAA,aAAA,CAAA,EAAiB;AAAA,QAClD,MAAA,EAAQ,MAAA;AAAA,QACR,OAAA,EAAS,EAAE,cAAA,EAAgB,kBAAA,EAAmB;AAAA,QAC9C,IAAA,EAAM,KAAK,SAAA,CAAU;AAAA,UACnB,KAAA;AAAA,UACA,MAAA;AAAA,UACA,MAAA,EAAQ,KAAA;AAAA,UACR,MAAA,EAAQ;AAAA,SACT,CAAA;AAAA,QACD,QAAQ,UAAA,CAAW;AAAA,OACpB,CAAA;AAED,MAAA,IAAI,CAAC,SAAS,EAAA,EAAI;AAChB,QAAA,MAAM,IAAI,KAAA,CAAM,CAAA,kBAAA,EAAqB,QAAA,CAAS,MAAM,CAAA,CAAE,CAAA;AAAA,MACxD;AAEA,MAAA,MAAM,MAAA,GAAS,MAAM,QAAA,CAAS,IAAA,EAAK;AACnC,MAAA,OAAO,MAAA,CAAO,QAAA;AAAA,IAChB,CAAA,SAAE;AACA,MAAA,YAAA,CAAa,SAAS,CAAA;AAAA,IACxB;AAAA,EACF,CAAA;AACF;AAhCgB,MAAA,CAAA,qBAAA,EAAA,uBAAA,CAAA;AAqCT,SAAS,kBACd,EAAA,EACA,MAAA,GAAuB,EAAC,EACxB,OAAA,GAKI,EAAC,EACoB;AACzB,EAAA,MAAM,GAAA,GAAM,OAAO,GAAA,IAAO,WAAA;AAC1B,EAAA,MAAM,KAAA,GAAQ,OAAO,KAAA,IAAS,aAAA;AAC9B,EAAA,MAAM,OAAA,GAAU,OAAO,OAAA,IAAW,eAAA;AAElC,EAAA,OAAO;AAAA,IACL,EAAA;AAAA,IACA,IAAA,EAAM,OAAA,CAAQ,IAAA,IAAQ,CAAA,QAAA,EAAW,KAAK,CAAA,CAAA,CAAA;AAAA,IACtC,IAAA,EAAM,QAAQ,IAAA,IAAQ,CAAA;AAAA;AAAA,IACtB,QAAA,EAAU,QAAQ,QAAA,IAAY,CAAA;AAAA;AAAA,IAC9B,OAAA,EAAS,IAAA;AAAA,IACT,gBAAA,EAAkB,KAAA;AAAA,IAClB,YAAA,EAAc;AAAA,MACZ,KAAA,EAAO,CAAC,SAAA,EAAW,QAAQ,CAAA;AAAA,MAC3B,UAAA,EAAY,GAAA;AAAA,MACZ,WAAA,EAAa,KAAA;AAAA,MACb,UAAA,EAAY,KAAA;AAAA,MACZ,KAAA,EAAO,KAAA;AAAA,MACP,GAAG,OAAA,CAAQ;AAAA,KACb;AAAA,IACA,OAAA,gCAAgB,IAAA,KAAkD;AAChE,MAAA,IAAI;AACF,QAAA,MAAM,UAAA,GAAa,IAAI,eAAA,EAAgB;AACvC,QAAA,MAAM,YAAY,UAAA,CAAW,MAAM,UAAA,CAAW,KAAA,IAAS,OAAO,CAAA;AAE9D,QAAA,MAAM,QAAA,GAAW,MAAM,KAAA,CAAM,CAAA,EAAG,GAAG,CAAA,aAAA,CAAA,EAAiB;AAAA,UAClD,MAAA,EAAQ,MAAA;AAAA,UACR,OAAA,EAAS,EAAE,cAAA,EAAgB,kBAAA,EAAmB;AAAA,UAC9C,IAAA,EAAM,KAAK,SAAA,CAAU;AAAA,YACnB,KAAA;AAAA,YACA,MAAA,EAAQ,iBAAiB,IAAI,CAAA;AAAA,YAC7B,MAAA,EAAQ;AAAA,WACT,CAAA;AAAA,UACD,QAAQ,UAAA,CAAW;AAAA,SACpB,CAAA;AAED,QAAA,YAAA,CAAa,SAAS,CAAA;AAEtB,QAAA,IAAI,CAAC,SAAS,EAAA,EAAI;AAChB,UAAA,OAAO;AAAA,YACL,OAAA,EAAS,KAAA;AAAA,YACT,KAAA,EAAO,CAAA,cAAA,EAAiB,QAAA,CAAS,MAAM,CAAA,CAAA;AAAA,YACvC,QAAA,EAAU,IAAA;AAAA,YACV,IAAA,EAAM;AAAA,WACR;AAAA,QACF;AAEA,QAAA,MAAM,MAAA,GAAS,MAAM,QAAA,CAAS,IAAA,EAAK;AACnC,QAAA,MAAM,MAAA,GAAS,MAAA,CAAO,QAAA,EAAU,IAAA,EAAK;AAGrC,QAAA,IAAI,MAAA,EAAQ,WAAA,EAAY,CAAE,UAAA,CAAW,WAAW,CAAA,EAAG;AACjD,UAAA,OAAO;AAAA,YACL,OAAA,EAAS,KAAA;AAAA,YACT,KAAA,EAAO,MAAA;AAAA,YACP,QAAA,EAAU,IAAA;AAAA,YACV,IAAA,EAAM;AAAA,WACR;AAAA,QACF;AAEA,QAAA,OAAO;AAAA,UACL,OAAA,EAAS,IAAA;AAAA,UACT,IAAA,EAAM,MAAA;AAAA,UACN,IAAA,EAAM;AAAA,SACR;AAAA,MACF,SAAS,KAAA,EAAO;AACd,QAAA,OAAO;AAAA,UACL,OAAA,EAAS,KAAA;AAAA,UACT,KAAA,EAAO,KAAA,YAAiB,KAAA,GAAQ,KAAA,CAAM,OAAA,GAAU,eAAA;AAAA,UAChD,QAAA,EAAU,IAAA;AAAA,UACV,IAAA,EAAM;AAAA,SACR;AAAA,MACF;AAAA,IACF,CAAA,EArDS,SAAA;AAAA,GAsDX;AACF;AApFgB,MAAA,CAAA,iBAAA,EAAA,mBAAA,CAAA;AAsFhB,SAAS,iBAAiB,IAAA,EAAyB;AACjD,EAAA,OAAO,CAAA;;AAAA,MAAA,EAED,KAAK,WAAW;;AAAA;AAAA,EAGtB,IAAA,CAAK,OAAA,CAAQ,KAAA,CAAM,CAAA,EAAG,GAAI,CAAC;;AAAA;AAAA;;AAAA,sBAAA,CAAA;AAM7B;AAZS,MAAA,CAAA,gBAAA,EAAA,kBAAA,CAAA","file":"ollama.js","sourcesContent":["/**\n * Ollama Provider Implementation\n *\n * Creates agents that use Ollama for LLM inference.\n * Ollama is free/self-hosted, ideal for trivial/simple tasks.\n */\n\nimport type {\n  AgentCapabilities,\n  AgentDefinition,\n  AgentResult,\n  AgentTask,\n  LLMEvaluator,\n} from '@agentic/triage';\n\nexport interface OllamaConfig {\n  /** Ollama API URL (default: http://localhost:11434) */\n  url?: string;\n  /** Model to use (default: qwen2.5-coder:32b) */\n  model?: string;\n  /** Request timeout in ms (default: 60000) */\n  timeout?: number;\n}\n\nconst DEFAULT_URL = 'http://localhost:11434';\nconst DEFAULT_MODEL = 'qwen2.5-coder:32b';\nconst DEFAULT_TIMEOUT = 60000;\n\n/**\n * Create an LLM evaluator function for Ollama\n * Use this with @agentic/triage's evaluateComplexity()\n */\nexport function createOllamaEvaluator(config: OllamaConfig = {}): LLMEvaluator {\n  const url = config.url ?? DEFAULT_URL;\n  const model = config.model ?? DEFAULT_MODEL;\n  const timeout = config.timeout ?? DEFAULT_TIMEOUT;\n\n  return async (prompt: string): Promise<string> => {\n    const controller = new AbortController();\n    const timeoutId = setTimeout(() => controller.abort(), timeout);\n\n    try {\n      const response = await fetch(`${url}/api/generate`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({\n          model,\n          prompt,\n          stream: false,\n          format: 'json',\n        }),\n        signal: controller.signal,\n      });\n\n      if (!response.ok) {\n        throw new Error(`Ollama API error: ${response.status}`);\n      }\n\n      const result = await response.json();\n      return result.response;\n    } finally {\n      clearTimeout(timeoutId);\n    }\n  };\n}\n\n/**\n * Create an Ollama-based agent for the registry\n */\nexport function createOllamaAgent(\n  id: string,\n  config: OllamaConfig = {},\n  options: {\n    name?: string;\n    cost?: number;\n    priority?: number;\n    capabilities?: Partial<AgentCapabilities>;\n  } = {}\n): AgentDefinition<string> {\n  const url = config.url ?? DEFAULT_URL;\n  const model = config.model ?? DEFAULT_MODEL;\n  const timeout = config.timeout ?? DEFAULT_TIMEOUT;\n\n  return {\n    id,\n    name: options.name ?? `Ollama (${model})`,\n    cost: options.cost ?? 0, // Free\n    priority: options.priority ?? 1, // High priority (try first)\n    enabled: true,\n    requiresApproval: false,\n    capabilities: {\n      tiers: ['trivial', 'simple'],\n      maxContext: 8000,\n      canCreatePR: false,\n      canExecute: false,\n      async: false,\n      ...options.capabilities,\n    },\n    execute: async (task: AgentTask): Promise<AgentResult<string>> => {\n      try {\n        const controller = new AbortController();\n        const timeoutId = setTimeout(() => controller.abort(), timeout);\n\n        const response = await fetch(`${url}/api/generate`, {\n          method: 'POST',\n          headers: { 'Content-Type': 'application/json' },\n          body: JSON.stringify({\n            model,\n            prompt: formatTaskPrompt(task),\n            stream: false,\n          }),\n          signal: controller.signal,\n        });\n\n        clearTimeout(timeoutId);\n\n        if (!response.ok) {\n          return {\n            success: false,\n            error: `Ollama error: ${response.status}`,\n            escalate: true,\n            cost: 0,\n          };\n        }\n\n        const result = await response.json();\n        const output = result.response?.trim();\n\n        // Check if Ollama is asking to escalate\n        if (output?.toUpperCase().startsWith('ESCALATE:')) {\n          return {\n            success: false,\n            error: output,\n            escalate: true,\n            cost: 0,\n          };\n        }\n\n        return {\n          success: true,\n          data: output,\n          cost: 0,\n        };\n      } catch (error) {\n        return {\n          success: false,\n          error: error instanceof Error ? error.message : 'Unknown error',\n          escalate: true,\n          cost: 0,\n        };\n      }\n    },\n  };\n}\n\nfunction formatTaskPrompt(task: AgentTask): string {\n  return `You are a code assistant. Complete this task:\n\nTASK: ${task.description}\n\nCONTEXT:\n${task.context.slice(0, 8000)}\n\nIf you can complete this task, provide the solution.\nIf this task is too complex, respond with: ESCALATE: <reason>\n\nProvide your response:`;\n}\n"]}